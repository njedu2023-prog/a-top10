#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
tools/debug_print_snapshot.py

âœ… æ—è·¯è°ƒè¯•å·¥å…·ï¼ˆç¨³å®šç‰ˆï¼‰ï¼š
- ä¸ä¿®æ”¹ä»»ä½• step æ–‡ä»¶
- å•ç‹¬è¿è¡Œå³å¯æ‰“å° pipeline å„é˜¶æ®µè¾“å‡º
- å…¼å®¹æ–‡ä»¶ç¼ºå¤± / ç©ºæ–‡ä»¶ / ç¼–ç å¼‚å¸¸ / åˆ†éš”ç¬¦å¼‚å¸¸ï¼ˆcsv/tsvï¼‰
- âœ… å…¼å®¹ Markdown è¡¨æ ¼ï¼ˆ.mdï¼‰ä¸ JSONï¼ˆ.jsonï¼‰è‡ªåŠ¨è§£ææ‰“å°

ç”¨æ³•ï¼š
  python tools/debug_print_snapshot.py
å¯é€‰ï¼š
  python tools/debug_print_snapshot.py --n 20
  python tools/debug_print_snapshot.py --dir outputs
  python tools/debug_print_snapshot.py --md outputs/debug_snapshot.md
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Any

import pandas as pd


# ===========================
# é»˜è®¤è¾“å‡ºç›®å½•
# ===========================
DEFAULT_OUTPUT_DIR = Path("outputs")


# ===========================
# é»˜è®¤æ–‡ä»¶æ˜ å°„ï¼ˆæ”¯æŒå¤šä¸ªå€™é€‰ï¼‰
# - æ¯ä¸ª title å¯¹åº”å¤šä¸ªå€™é€‰æ–‡ä»¶å/é€šé…ç¬¦
# - ä¼šè‡ªåŠ¨é€‰æ‹©â€œç¬¬ä¸€ä¸ªå­˜åœ¨çš„â€ï¼Œæˆ–é€šé…ç¬¦åŒ¹é…åˆ°çš„â€œæœ€æ–°æ–‡ä»¶â€
# ===========================
DEFAULT_FILES: Dict[str, List[str]] = {
    "Step2 Candidate Poolï¼ˆå€™é€‰æ¶¨åœæ± ï¼‰": [
        "step2_candidates.csv",
        "step2_candidates.tsv",
        "step2_candidates.md",
        "step2_candidates.json",
    ],
    "Step3 StrengthScore è¾“å‡ºï¼ˆå¼ºåº¦è¯„åˆ†ï¼‰": [
        "step3_strength.csv",
        "step3_strength.tsv",
        "step3_strength.md",
        "step3_strength.json",
    ],
    "Final Top10 è¾“å‡ºï¼ˆæœ€ç»ˆæ¦œå•ï¼‰": [
        # ä½ ä¹‹å‰è®¾æƒ³çš„
        "predict_top10_latest.csv",
        "predict_top10_latest.md",
        "predict_top10_latest.json",
        # repo é‡Œå¸¸è§çš„
        "latest.md",
        "predict_top10_*.csv",
        "predict_top10_*.md",
        "predict_top10_*.json",
    ],
}


# ===========================
# å·¥å…·ï¼šé€‰æ‹©ä¸€ä¸ªå¯ç”¨æ–‡ä»¶ï¼ˆæ”¯æŒé€šé…ç¬¦å–æœ€æ–°ï¼‰
# ===========================
def _resolve_existing_file(out_dir: Path, patterns: List[str]) -> Optional[Path]:
    candidates: List[Path] = []
    for p in patterns:
        if any(ch in p for ch in ["*", "?", "["]):
            candidates.extend(sorted(out_dir.glob(p)))
        else:
            candidates.append(out_dir / p)

    existing = [x for x in candidates if x.exists() and x.is_file() and x.stat().st_size > 0]
    if not existing:
        return None

    # å–æœ€æ–°ï¼šä¼˜å…ˆæŒ‰ mtimeï¼Œå…¶æ¬¡æŒ‰åå­—ï¼ˆpredict_top10_YYYYMMDD.xxx ä¹Ÿèƒ½æ­£å¸¸æ’åºï¼‰
    existing.sort(key=lambda x: (x.stat().st_mtime, x.name))
    return existing[-1]


# ===========================
# è¯» CSV/TSVï¼šå°½é‡ç¨³
# ===========================
def _read_csv_safely(path: Path) -> Tuple[Optional[pd.DataFrame], str]:
    """
    å°½åŠ›è¯»å– CSV/TSVï¼š
    - è‡ªåŠ¨å°è¯• utf-8-sig / utf-8 / gbk
    - è‡ªåŠ¨å°è¯•åˆ†éš”ç¬¦ï¼š,  \\t  ;
    è¿”å› (df, msg)ã€‚df ä¸º None è¡¨ç¤ºå¤±è´¥ã€‚
    """
    if not path.exists():
        return None, f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}"

    if path.stat().st_size == 0:
        return None, f"âš ï¸ æ–‡ä»¶ä¸ºç©ºï¼š{path}"

    encodings = ["utf-8-sig", "utf-8", "gbk"]
    seps = [",", "\t", ";"]

    last_err: Optional[Exception] = None
    for enc in encodings:
        for sep in seps:
            try:
                df = pd.read_csv(path, encoding=enc, sep=sep, engine="python")

                # æœ‰äº›æ–‡ä»¶è¯»å‡ºæ¥åªæœ‰ 1 åˆ—ä¸”åˆ—ååƒæ•´è¡Œï¼Œæ¢åˆ†éš”ç¬¦å†è¯•
                if df.shape[1] == 1 and df.columns.size == 1:
                    col0 = str(df.columns[0])
                    if any(s in col0 for s in [",", "\t", ";"]):
                        continue

                return df, f"âœ… è¯»å–æˆåŠŸï¼š{path.name}ï¼ˆencoding={enc}, sep={repr(sep)}ï¼‰"
            except Exception as e:
                last_err = e

    return None, f"âŒ è¯»å–å¤±è´¥ï¼š{path}ï¼ˆæœ€åé”™è¯¯ï¼š{last_err}ï¼‰"


# ===========================
# è¯» Markdown è¡¨æ ¼ï¼šå°½é‡ç¨³
# ===========================
def _read_markdown_table(path: Path) -> Tuple[Optional[pd.DataFrame], str]:
    """
    è§£æ markdown é‡Œçš„ pipe è¡¨æ ¼ï¼š
      | a | b |
      |---|---|
      | 1 | 2 |
    æ‰¾åˆ°ç¬¬ä¸€å¼ è¡¨å°±è¯»ã€‚
    """
    try:
        text = path.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        return None, f"âŒ è¯»å–å¤±è´¥ï¼ˆmdï¼‰ï¼š{path}ï¼ˆé”™è¯¯ï¼š{e}ï¼‰"

    lines = [ln.rstrip("\n") for ln in text.splitlines()]
    table_start = -1

    # æ‰¾åˆ°è¡¨å¤´ + åˆ†éš”çº¿
    for i in range(len(lines) - 1):
        if "|" in lines[i] and "|" in lines[i + 1]:
            sep_line = lines[i + 1].strip()
            if re.match(r"^\s*\|?\s*[-: ]+\|\s*[-:| ]+\|?\s*$", sep_line):
                table_start = i
                break

    if table_start < 0:
        # æ²¡è¡¨æ ¼å°±å½“æ™®é€šæ–‡æœ¬
        return None, f"âš ï¸ æœªå‘ç° Markdown è¡¨æ ¼ï¼š{path.name}"

    # æ”¶é›†è¿ç»­çš„è¡¨æ ¼è¡Œ
    tbl = []
    j = table_start
    while j < len(lines) and ("|" in lines[j]) and lines[j].strip():
        tbl.append(lines[j].strip())
        j += 1

    if len(tbl) < 3:
        return None, f"âš ï¸ Markdown è¡¨æ ¼ä¸å®Œæ•´ï¼š{path.name}"

    header = tbl[0]
    sep = tbl[1]
    rows = tbl[2:]

    # æ¸…æ´—ï¼šå»æ‰é¦–å°¾ |
    def split_row(r: str) -> List[str]:
        r = r.strip()
        if r.startswith("|"):
            r = r[1:]
        if r.endswith("|"):
            r = r[:-1]
        return [c.strip() for c in r.split("|")]

    cols = split_row(header)
    data = [split_row(r) for r in rows]

    # å¯¹é½åˆ—æ•°
    max_len = max(len(cols), *(len(r) for r in data))
    cols = cols + [""] * (max_len - len(cols))
    fixed = []
    for r in data:
        if len(r) < max_len:
            r = r + [""] * (max_len - len(r))
        fixed.append(r[:max_len])

    try:
        df = pd.DataFrame(fixed, columns=cols)
        # å»æ‰ç©ºåˆ—åï¼ˆå¾ˆå¸¸è§ï¼‰
        df.columns = [c if c else f"col_{i}" for i, c in enumerate(df.columns)]
        return df, f"âœ… è¯»å–æˆåŠŸï¼š{path.name}ï¼ˆmarkdown tableï¼‰"
    except Exception as e:
        return None, f"âŒ è§£æ Markdown è¡¨æ ¼å¤±è´¥ï¼š{path.name}ï¼ˆé”™è¯¯ï¼š{e}ï¼‰"


# ===========================
# è¯» JSONï¼šå°½é‡ç¨³
# ===========================
def _read_json_safely(path: Path) -> Tuple[Optional[pd.DataFrame], str]:
    try:
        raw = path.read_text(encoding="utf-8", errors="ignore")
        obj = json.loads(raw)

        if isinstance(obj, list):
            df = pd.json_normalize(obj)
            return df, f"âœ… è¯»å–æˆåŠŸï¼š{path.name}ï¼ˆjson listï¼‰"

        if isinstance(obj, dict):
            # å¸¸è§ï¼šdict é‡ŒåŒ…äº† topN / full / data ç­‰
            for key in ["topN", "topn", "data", "items", "rows", "result", "full"]:
                if key in obj and isinstance(obj[key], list):
                    df = pd.json_normalize(obj[key])
                    return df, f"âœ… è¯»å–æˆåŠŸï¼š{path.name}ï¼ˆjson dict[{key}]ï¼‰"
            # å…œåº•ï¼šæŠŠ dict å±•å¹³
            df = pd.json_normalize(obj)
            return df, f"âœ… è¯»å–æˆåŠŸï¼š{path.name}ï¼ˆjson dictï¼‰"

        return None, f"âš ï¸ JSON ç»“æ„ä¸æ”¯æŒï¼ˆä¸æ˜¯ list/dictï¼‰ï¼š{path.name}"
    except Exception as e:
        return None, f"âŒ è¯»å–å¤±è´¥ï¼ˆjsonï¼‰ï¼š{path.name}ï¼ˆé”™è¯¯ï¼š{e}ï¼‰"


# ===========================
# æ€»å…¥å£ï¼šæŒ‰åç¼€è‡ªåŠ¨é€‰æ‹©è¯»å–æ–¹å¼
# ===========================
def _read_any_safely(path: Path) -> Tuple[Optional[pd.DataFrame], str]:
    suf = path.suffix.lower()
    if suf in [".csv", ".tsv"]:
        return _read_csv_safely(path)
    if suf in [".md", ".markdown"]:
        return _read_markdown_table(path)
    if suf == ".json":
        return _read_json_safely(path)
    # å…œåº•ï¼šå°è¯•æŒ‰ csv
    return _read_csv_safely(path)


# ===========================
# æ‰“å°ï¼šå¯è¯»ã€å¯æ§
# ===========================
def _maybe_sort(df: pd.DataFrame) -> pd.DataFrame:
    """
    å¦‚æœæœ‰å¸¸è§åˆ—ï¼Œå°±åšä¸€ä¸ªè½»é‡æ’åºï¼Œä¾¿äºè‚‰çœ¼å¯¹æ¯”ã€‚
    è§„åˆ™ï¼š
    - rank/æ’å/ts_code/è‚¡ç¥¨ä»£ç ï¼šå‡åº
    - score/prob/StrengthScore ç­‰ï¼šé™åº
    """
    if df is None or df.empty:
        return df

    prefer_cols = [
        "rank", "æ’å",
        "ts_code", "è‚¡ç¥¨ä»£ç ",
        "StrengthScore", "å¼ºåº¦å¾—åˆ†", "å¼ºåº¦è¯„åˆ†",
        "prob", "æ¶¨åœæ¦‚ç‡",
        "_score", "score", "ç»¼åˆå¾—åˆ†",
    ]

    sort_cols = [c for c in prefer_cols if c in df.columns]
    if not sort_cols:
        return df

    seen = set()
    uniq_cols = []
    for c in sort_cols:
        if c not in seen:
            uniq_cols.append(c)
            seen.add(c)

    def _asc_for(col: str) -> bool:
        key = col.lower()
        if col in ("rank", "æ’å", "ts_code", "è‚¡ç¥¨ä»£ç "):
            return True
        if "rank" in key or "code" in key:
            return True
        return False  # å…¶ä½™é»˜è®¤é™åº

    ascending = [_asc_for(c) for c in uniq_cols]

    try:
        return df.sort_values(by=uniq_cols, ascending=ascending)
    except Exception:
        return df


def _print_df(title: str, df: Optional[pd.DataFrame], n: int = 10, max_colwidth: int = 32) -> str:
    lines = []
    lines.append("\n" + "=" * 78)
    lines.append(f"ğŸ“Œ {title}")
    lines.append("=" * 78)

    if df is None:
        lines.append("ï¼ˆæ— æ•°æ® / è§£æå¤±è´¥ / æ–‡ä»¶ä¸å­˜åœ¨ï¼‰")
        out = "\n".join(lines)
        print(out)
        return out

    if df.empty:
        lines.append("ï¼ˆDataFrame ä¸ºç©ºï¼‰")
        out = "\n".join(lines)
        print(out)
        return out

    df2 = _maybe_sort(df.copy())

    def _truncate(x):
        s = "" if pd.isna(x) else str(x)
        if len(s) > max_colwidth:
            return s[: max_colwidth - 1] + "â€¦"
        return s

    try:
        for col in df2.columns:
            if df2[col].dtype == "object":
                df2[col] = df2[col].map(_truncate)
    except Exception:
        pass

    with pd.option_context(
        "display.max_rows", n,
        "display.max_columns", 200,
        "display.width", 220,
        "display.max_colwidth", max_colwidth,
    ):
        head_txt = df2.head(n).to_string(index=False)
        lines.append(head_txt)
        lines.append(f"\nâœ… æ€»è¡Œæ•°: {len(df2)}")
        lines.append(f"âœ… åˆ—æ•°: {len(df2.columns)}")
        lines.append(f"âœ… åˆ—å: {list(df2.columns)}")

    out = "\n".join(lines)
    print(out)
    return out


def _to_markdown_block(title: str, df: Optional[pd.DataFrame], n: int = 10) -> str:
    md = []
    md.append(f"\n## {title}\n")

    if df is None:
        md.append("> ï¼ˆæ— æ•°æ® / è§£æå¤±è´¥ / æ–‡ä»¶ä¸å­˜åœ¨ï¼‰\n")
        return "".join(md)

    if df.empty:
        md.append("> ï¼ˆDataFrame ä¸ºç©ºï¼‰\n")
        return "".join(md)

    df2 = _maybe_sort(df.copy()).head(n)
    try:
        md.append(df2.to_markdown(index=False))
        md.append("\n")
        md.append(f"\n- æ€»è¡Œæ•°ï¼š{len(df)}\n")
        md.append(f"- åˆ—åï¼š{list(df.columns)}\n")
    except Exception:
        md.append("```text\n")
        md.append(df2.to_string(index=False))
        md.append("\n```\n")
        md.append(f"\n- æ€»è¡Œæ•°ï¼š{len(df)}\n")
        md.append(f"- åˆ—åï¼š{list(df.columns)}\n")

    return "".join(md)


# ===========================
# ä¸»å…¥å£
# ===========================
def main() -> int:
    parser = argparse.ArgumentParser(description="Top10 ç³»ç»Ÿæ—è·¯è°ƒè¯•æ‰“å°å™¨ï¼ˆç¨³å®šç‰ˆï¼‰")
    parser.add_argument("--dir", type=str, default=str(DEFAULT_OUTPUT_DIR), help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ outputsï¼‰")
    parser.add_argument("--n", type=int, default=10, help="æ¯ä¸ªè¡¨æ‰“å°å‰ N è¡Œï¼ˆé»˜è®¤ 10ï¼‰")
    parser.add_argument("--max-colwidth", type=int, default=32, help="å­—ç¬¦ä¸²åˆ—æœ€å¤§æ˜¾ç¤ºå®½åº¦ï¼ˆé»˜è®¤ 32ï¼‰")
    parser.add_argument("--md", type=str, default="", help="å¯é€‰ï¼šå†™å…¥ markdown æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    out_dir = Path(args.dir)
    n = max(1, int(args.n))
    max_colwidth = max(8, int(args.max_colwidth))

    print("\nâœ… Top10 ç³»ç»Ÿæ—è·¯è°ƒè¯•æ‰“å°å™¨å¯åŠ¨...\n")
    print(f"ğŸ“ è¾“å‡ºç›®å½•ï¼š{out_dir.resolve()}")
    print(f"ğŸ” æ¯è¡¨æ˜¾ç¤ºè¡Œæ•°ï¼š{n}")
    print(f"ğŸ“ æœ€å¤§åˆ—å®½ï¼š{max_colwidth}\n")

    if not out_dir.exists():
        print(f"âš ï¸ è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼š{out_dir}")
        print("   ä½ å¯ä»¥å…ˆè·‘ä¸€æ¬¡ pipeline ç”Ÿæˆ outputsï¼Œæˆ–æŒ‡å®šæ­£ç¡® --dir\n")

    md_parts: List[str] = []
    if args.md:
        md_parts.append(f"# Top10 Debug Snapshot\n\n- è¾“å‡ºç›®å½•ï¼š`{out_dir}`\n- æ¯è¡¨æ˜¾ç¤ºï¼šå‰ {n} è¡Œ\n")

    for title, patterns in DEFAULT_FILES.items():
        resolved = _resolve_existing_file(out_dir, patterns)
        if resolved is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ–‡ä»¶ï¼š{title}ï¼ˆå°è¯•è¿‡ï¼š{patterns}ï¼‰")
            df = None
            _print_df(title, df, n=n, max_colwidth=max_colwidth)
            if args.md:
                md_parts.append(_to_markdown_block(title, df, n=n))
            continue

        df, msg = _read_any_safely(resolved)
        print(msg + f"  -> {resolved}")
        _print_df(title, df, n=n, max_colwidth=max_colwidth)

        if args.md:
            md_parts.append(_to_markdown_block(title, df, n=n))

    if args.md:
        md_path = Path(args.md)
        try:
            md_path.parent.mkdir(parents=True, exist_ok=True)
            md_path.write_text("".join(md_parts), encoding="utf-8")
            print(f"\nğŸ“ å·²å†™å…¥ Markdownï¼š{md_path.resolve()}\n")
        except Exception as e:
            print(f"\nâš ï¸ å†™å…¥ Markdown å¤±è´¥ï¼š{e}\n")

    print("\nâœ… æ—è·¯è°ƒè¯•ç»“æŸã€‚\n")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
