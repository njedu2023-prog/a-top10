name: Run Top10 Engine (Auto Daily)

on:
  schedule:
    - cron: "5 11 * * 1-5"
    - cron: "35 11 * * 1-5"

  workflow_dispatch:
    inputs:
      trade_date:
        description: "指定交易日 YYYYMMDD（留空=自动寻找最新合格日期）"
        required: false
        type: string

permissions:
  contents: write

concurrency:
  group: top10-daily
  cancel-in-progress: false

defaults:
  run:
    shell: bash

jobs:
  run:
    runs-on: ubuntu-latest

    env:
      TZ: Asia/Shanghai
      PYTHONUTF8: "1"

      # secret 名为 A_TOP10_TOKEN，这里桥接到程序读取的 TUSHARE_TOKEN
      TUSHARE_TOKEN: ${{ secrets.A_TOP10_TOKEN }}

      # TOP3 数据仓库
      DATA_GITHUB_USER: njedu2023-prog
      DATA_REPO: a-share-top3-data
      DATA_BRANCH: main

      # 与 configs/default.yml 对齐
      WAREHOUSE_ROOT: _warehouse
      RAW_DIR: data/raw

      # 必须存在的关键文件（缺任何一个都失败）
      REQUIRED_FILES: >-
        daily.csv
        daily_basic.csv
        limit_list_d.csv
        hot_boards.csv
        top_list.csv
        limit_up_tags.csv

      LOOKBACK_DAYS: "20"
      PROBE_FILE: "daily.csv"

    steps:
      - name: Checkout engine repo (a-top10)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install -U tabulate
          pip install -U lightgbm

      # Resolve trade_date (P0 gate: open_times & seal_amount required)
      - name: Resolve trade_date (manual or auto, with P0 gate)
        id: resolve_date
        run: |
          set -euo pipefail

          MANUAL="${{ github.event.inputs.trade_date || '' }}"
          if [ -n "$MANUAL" ]; then
            echo "Use manual trade_date=$MANUAL"
            echo "trade_date=$MANUAL" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "Auto resolve latest snapshot trade_date (P0: open_times & seal_amount required)"
          today="$(date +%Y%m%d)"
          base="https://raw.githubusercontent.com/${DATA_GITHUB_USER}/${DATA_REPO}/${DATA_BRANCH}/${RAW_DIR}"

          found=""
          for i in $(seq 0 "$LOOKBACK_DAYS"); do
            d="$(date -d "${today} -${i} day" +%Y%m%d)"
            y="${d:0:4}"

            url_probe="${base}/${y}/${d}/${PROBE_FILE}"
            code="$(curl -s -o /dev/null -w "%{http_code}" -I "$url_probe" || true)"
            echo "[probe] ${d} daily => ${code}"
            if [ "$code" != "200" ]; then
              continue
            fi

            tmp="$(mktemp)"
            url_ll="${base}/${y}/${d}/limit_list_d.csv"
            if ! curl -fsSL --retry 2 --retry-delay 1 -o "$tmp" "$url_ll"; then
              echo "[probe] ${d} limit_list_d.csv download FAIL"
              rm -f "$tmp" || true
              continue
            fi

            ok="0"
            if python -c "import pandas as pd,sys; df=pd.read_csv(sys.argv[1]); must=['open_times','seal_amount']; miss=[c for c in must if c not in df.columns]; assert not miss, f'missing cols {miss}'; all_empty=lambda s: s.isna().all() or (s.astype(str).str.strip()=='').all(); bad=[c for c in must if all_empty(df[c])]; assert not bad, f'cols all-empty {bad}'" "$tmp"; then
              ok="1"
            fi

            rm -f "$tmp" || true

            if [ "$ok" = "1" ]; then
              found="$d"
              echo "[pick] trade_date=$found (P0 gate PASS)"
              break
            else
              echo "[probe] ${d} P0 gate FAIL -> fallback"
            fi
          done

          if [ -z "$found" ]; then
            echo "ERROR: cannot find any snapshot within last ${LOOKBACK_DAYS} days that passes P0 gate (open_times/seal_amount)"
            exit 2
          fi

          echo "Resolved trade_date=$found"
          echo "trade_date=$found" >> "$GITHUB_OUTPUT"

      # Sync ONLY the resolved trade_date directory into _warehouse (system contract)
      - name: Sync TOP3 snapshot to _warehouse (ONLY trade_date dir)
        id: sync_top3
        run: |
          set -euo pipefail

          trade_date="${{ steps.resolve_date.outputs.trade_date }}"
          year="${trade_date:0:4}"

          src_dir="${RAW_DIR}/${year}/${trade_date}"
          dest_dir="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}"

          rm -rf _tmp_top3 || true
          git clone --depth 1 --filter=blob:none --sparse \
            --branch "${DATA_BRANCH}" \
            "https://github.com/${DATA_GITHUB_USER}/${DATA_REPO}.git" _tmp_top3

          git -C _tmp_top3 sparse-checkout init --cone
          git -C _tmp_top3 sparse-checkout set "$src_dir"

          if [ ! -d "_tmp_top3/$src_dir" ]; then
            echo "ERROR: source snapshot dir missing in TOP3 repo: _tmp_top3/$src_dir"
            exit 10
          fi

          mkdir -p "$dest_dir"
          rsync -a --delete "_tmp_top3/$src_dir/" "$dest_dir/"

          SRC_SHA="$(git -C _tmp_top3 rev-parse HEAD)"
          mkdir -p outputs
          echo "{\"source_repo\":\"${DATA_GITHUB_USER}/${DATA_REPO}\",\"source_branch\":\"${DATA_BRANCH}\",\"source_sha\":\"${SRC_SHA}\",\"trade_date\":\"${trade_date}\",\"synced_dir\":\"${src_dir}\",\"synced_at_utc\":\"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"}" > outputs/warehouse_sync_meta.json

          echo "synced_dir=$dest_dir" >> "$GITHUB_OUTPUT"
          echo "src_sha=$SRC_SHA" >> "$GITHUB_OUTPUT"

      # Debug: print header/columns for limit_list_d.csv (proof of sync)
      - name: Debug synced limit_list_d (header + columns)
        run: |
          set -euo pipefail
          td="${{ steps.resolve_date.outputs.trade_date }}"
          y="${td:0:4}"
          f="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${y}/${td}/limit_list_d.csv"
          echo "[debug] file=$f"
          head -n 2 "$f" || true
          python -c "import pandas as pd; df=pd.read_csv(r'$f'); print(df.columns.tolist()); print('rows=', len(df))"

      # Quality gate (required files + key columns)
      - name: Quality gate (required files + key columns)
        run: |
          set -euo pipefail

          trade_date="${{ steps.resolve_date.outputs.trade_date }}"
          year="${trade_date:0:4}"
          snap_dir="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}"

          missing_required=""
          for rf in $REQUIRED_FILES; do
            p="${snap_dir}/${rf}"
            if [ ! -s "$p" ]; then
              echo "ERROR: required snapshot file missing/empty: $rf"
              missing_required="${missing_required} ${rf}"
            fi
          done
          if [ -n "$missing_required" ]; then
            echo "ERROR: required snapshot files missing:${missing_required}"
            exit 21
          fi

          python -c "import pandas as pd,os,sys; snap=sys.argv[1]; df=pd.read_csv(os.path.join(snap,'limit_list_d.csv')); must=['limit_type','up_limit','down_limit','open_times','seal_amount']; miss=[c for c in must if c not in df.columns]; assert not miss, f'missing cols {miss}'; all_empty=lambda s: s.isna().all() or (s.astype(str).str.strip()=='').all(); bad=[c for c in must if all_empty(df[c])]; assert not bad, f'cols all-empty {bad}'" "$snap_dir"

      # Upload snapshot dir as artifact (optional but recommended as audit proof)
      - name: Upload synced snapshot dir (artifact)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: top3_snapshot_${{ steps.resolve_date.outputs.trade_date }}_${{ github.run_id }}
          path: ${{ steps.sync_top3.outputs.synced_dir }}
          if-no-files-found: error
          retention-days: 30

      - name: Run Top10 Engine + Step7
        env:
          TRADE_DATE: ${{ steps.resolve_date.outputs.trade_date }}
        run: |
          set -euo pipefail
          if [ -f "run.py" ]; then
            python run.py
          else
            python -m a_top10
          fi
          python -m a_top10.steps.step7_self_learning

      # ✅ Stamp (single file overwrite)
      - name: Stamp last run (always)
        if: always()
        run: |
          set -euo pipefail
          td="${{ steps.resolve_date.outputs.trade_date }}"
          rid="${{ github.run_id }}"
          sha="${{ github.sha }}"
          mkdir -p outputs/learning
          echo "trade_date=${td} run_id=${rid} commit_sha=${sha} utc=$(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "outputs/learning/_last_run.txt"

      # ✅ Commit & push: outputs + models + _warehouse(today)
      - name: Commit & push outputs + models + _warehouse(today) (always)
        if: always()
        run: |
          set -euo pipefail

          td="${{ steps.resolve_date.outputs.trade_date }}"
          y="${td:0:4}"
          rid="${{ github.run_id }}"
          snap_dir="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${y}/${td}"

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          rm -rf _tmp_top3 || true

          git add -A outputs models
          git add -A "$snap_dir"

          if [ -z "$(git status --porcelain)" ]; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "auto: run top10 ${td} (run=${rid})" || true
          git pull --rebase origin main || true
          git push || true
