name: Run Top10 Engine (Auto Daily)

on:
  schedule:
    # ✅ GitHub Actions 的 cron 使用 UTC 时区
    # ✅ 北京时间 Asia/Shanghai = UTC+8
    # ✅ 周一至周五（1-5）：
    #    - 19:05 北京时间  => 11:05 UTC
    #    - 19:35 北京时间  => 11:35 UTC
    - cron: "5 11 * * 1-5"
    - cron: "35 11 * * 1-5"

  # 手动运行：可指定 trade_date
  workflow_dispatch:
    inputs:
      trade_date:
        description: "指定交易日 YYYYMMDD（留空=自动寻找最新）"
        required: false
        type: string

permissions:
  contents: write

concurrency:
  group: top10-daily
  cancel-in-progress: false

jobs:
  run:
    runs-on: ubuntu-latest

    env:
      TZ: Asia/Shanghai
      PYTHONUTF8: "1"

      # ✅ 关键：secret 名为 A_TOP10_TOKEN，这里桥接到程序读取的 TUSHARE_TOKEN
      TUSHARE_TOKEN: ${{ secrets.A_TOP10_TOKEN }}

      # ---- 数据仓库信息（TOP3 数据仓库）----
      DATA_GITHUB_USER: njedu2023-prog
      DATA_REPO: a-share-top3-data
      DATA_BRANCH: main

      # ---- 与 configs/default.yml 对齐（warehouse_root / repo_name / raw_dir）----
      WAREHOUSE_ROOT: _warehouse
      RAW_DIR: data/raw

      # ---- ✅ 全量快照文件清单（按 data/raw/YYYY/YYYYMMDD/ 内实际文件名）----
      FILES: >-
        _meta.json
        daily.csv
        daily_basic.csv
        hot_boards.csv
        limit_break_d.csv
        limit_list_d.csv
        limit_up_tags.csv
        moneyflow_hsgt.csv
        namechange.csv
        stk_limit.csv
        stock_basic.csv
        top_list.csv

      # ---- ✅ 必须存在的关键文件（缺任何一个都直接失败）----
      REQUIRED_FILES: >-
        daily.csv
        daily_basic.csv
        limit_list_d.csv
        hot_boards.csv
        top_list.csv
        limit_up_tags.csv

      # ---- 自动回溯天数（找最近一个“可用”的交易日目录）----
      LOOKBACK_DAYS: "20"

      # ---- 用于探测数据是否存在的哨兵文件 ----
      PROBE_FILE: "daily.csv"

    steps:
      - name: Checkout engine repo (a-top10)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

          # ✅ writers 需要 tabulate
          pip install -U tabulate

          # ✅ Step7 需要 lightgbm（否则永远无法生成 step5_lgbm.joblib）
          pip install -U lightgbm

      # ============================================================
      # ✅ Resolve trade_date: P0 gate moved here (open_times & seal_amount required)
      # - manual: respect user input
      # - auto: find latest date that passes P0 gate
      # ============================================================
      - name: Resolve trade_date (manual or auto, with P0 gate)
        id: resolve_date
        shell: bash
        run: |
          set -euo pipefail

          MANUAL="${{ github.event.inputs.trade_date || '' }}"
          if [ -n "$MANUAL" ]; then
            echo "Use manual trade_date=$MANUAL"
            echo "trade_date=$MANUAL" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "Auto resolve latest snapshot trade_date (P0 gate: open_times & seal_amount required)"
          today="$(date +%Y%m%d)"
          base="https://raw.githubusercontent.com/${DATA_GITHUB_USER}/${DATA_REPO}/${DATA_BRANCH}/${RAW_DIR}"

          found=""

          for i in $(seq 0 "$LOOKBACK_DAYS"); do
            d="$(date -d "${today} -${i} day" +%Y%m%d)"
            y="${d:0:4}"

            url_probe="${base}/${y}/${d}/${PROBE_FILE}"
            code="$(curl -s -o /dev/null -w "%{http_code}" -I "$url_probe" || true)"
            echo "[probe] ${d} daily => ${code}"
            if [ "$code" != "200" ]; then
              continue
            fi

            # P0 gate: limit_list_d must contain open_times & seal_amount and not all-empty
            tmp="$(mktemp -t limit_list_${d}_XXXX.csv)"
            url_ll="${base}/${y}/${d}/limit_list_d.csv"
            if ! curl -fsSL --retry 2 --retry-delay 1 -o "$tmp" "$url_ll"; then
              echo "[probe] ${d} limit_list_d.csv download FAIL"
              rm -f "$tmp" || true
              continue
            fi

            ok="0"
            if python - "$tmp" <<'PY'
import sys
import pandas as pd

p = sys.argv[1]
df = pd.read_csv(p)

must = ["open_times", "seal_amount"]
miss = [c for c in must if c not in df.columns]
if miss:
    raise SystemExit(f"[probe] missing cols {miss}")

def all_empty(s):
    if s.isna().all():
        return True
    return (s.astype(str).str.strip() == "").all()

bad = [c for c in must if all_empty(df[c])]
if bad:
    raise SystemExit(f"[probe] cols all-empty {bad}")

print("[probe] PASS open_times & seal_amount")
PY
            then
              ok="1"
            else
              ok="0"
            fi

            rm -f "$tmp" || true

            if [ "$ok" = "1" ]; then
              found="$d"
              echo "[pick] trade_date=$found (P0 gate PASS)"
              break
            else
              echo "[probe] ${d} P0 gate FAIL -> fallback"
            fi
          done

          if [ -z "$found" ]; then
            echo "ERROR: cannot find any snapshot within last ${LOOKBACK_DAYS} days that passes P0 gate (open_times/seal_amount)"
            exit 2
          fi

          echo "Resolved trade_date=$found"
          echo "trade_date=$found" >> "$GITHUB_OUTPUT"

      # ============================================================
      # ✅ Sync ONLY the resolved trade_date directory via sparse checkout
      # ============================================================
      - name: Sync TOP3 snapshot (ONLY trade_date dir)
        shell: bash
        run: |
          set -euo pipefail

          trade_date="${{ steps.resolve_date.outputs.trade_date }}"
          year="${trade_date:0:4}"

          src_dir="${RAW_DIR}/${year}/${trade_date}"
          dest_dir="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}"

          echo "[sync] trade_date=$trade_date"
          echo "[sync] src_dir=$src_dir"
          echo "[sync] dest_dir=$dest_dir"

          rm -rf _tmp_top3
          git clone --depth 1 --filter=blob:none --sparse \
            --branch "${DATA_BRANCH}" \
            "https://github.com/${DATA_GITHUB_USER}/${DATA_REPO}.git" _tmp_top3

          git -C _tmp_top3 sparse-checkout init --cone
          git -C _tmp_top3 sparse-checkout set "$src_dir"

          if [ ! -d "_tmp_top3/$src_dir" ]; then
            echo "ERROR: source snapshot dir missing in TOP3 repo: _tmp_top3/$src_dir"
            exit 10
          fi

          mkdir -p "$dest_dir"
          rsync -a --delete "_tmp_top3/$src_dir/" "$dest_dir/"

          SRC_SHA="$(git -C _tmp_top3 rev-parse HEAD)"
          mkdir -p outputs
          cat > outputs/warehouse_sync_meta.json <<EOF
          {
            "source_repo": "${DATA_GITHUB_USER}/${DATA_REPO}",
            "source_branch": "${DATA_BRANCH}",
            "source_sha": "${SRC_SHA}",
            "trade_date": "${trade_date}",
            "synced_dir": "${src_dir}",
            "synced_at_utc": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          }
          EOF

          echo "[sync] done, source_sha=${SRC_SHA}"

      # ============================================================
      # ✅ Quality gate for resolved trade_date (P0 hard gate)
      # ============================================================
      - name: Quality gate (required files + key columns)
        shell: bash
        run: |
          set -euo pipefail

          trade_date="${{ steps.resolve_date.outputs.trade_date }}"
          year="${trade_date:0:4}"
          snap_dir="${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}"

          echo "[gate] snapshot dir: $snap_dir"
          if [ ! -d "$snap_dir" ]; then
            echo "ERROR: snapshot dir missing: $snap_dir"
            exit 20
          fi

          missing_required=""
          for rf in $REQUIRED_FILES; do
            p="${snap_dir}/${rf}"
            if [ ! -s "$p" ]; then
              echo "ERROR: required snapshot file missing/empty: $rf"
              missing_required="${missing_required} ${rf}"
            fi
          done

          if [ -n "$missing_required" ]; then
            echo "ERROR: required snapshot files missing:${missing_required}"
            exit 21
          fi

          python - "$snap_dir" <<'PY'
import sys
import os
import pandas as pd

snap_dir = sys.argv[1]
f = os.path.join(snap_dir, "limit_list_d.csv")
df = pd.read_csv(f)

must_cols = ["limit_type", "up_limit", "down_limit", "open_times", "seal_amount"]
miss = [c for c in must_cols if c not in df.columns]
if miss:
    raise SystemExit(f"[gate] limit_list_d.csv missing columns: {miss}")

def all_empty(s):
    if s.isna().all():
        return True
    return (s.astype(str).str.strip() == "").all()

bad = [c for c in must_cols if all_empty(df[c])]
if bad:
    raise SystemExit(f"[gate] limit_list_d.csv columns all-empty: {bad}")

print("[gate] PASS: required files + limit_list_d key columns OK")
PY

      - name: Debug warehouse (list)
        shell: bash
        run: |
          set -euo pipefail

          trade_date="${{ steps.resolve_date.outputs.trade_date }}"
          year="${trade_date:0:4}"

          echo "====== LIST _warehouse (top) ======"
          ls -la _warehouse || true

          echo "====== LIST snapshot dir ======"
          ls -la "${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}" || true

          echo "====== CHECK key files (size) ======"
          du -h "${WAREHOUSE_ROOT}/${DATA_REPO}/${RAW_DIR}/${year}/${trade_date}"/* || true

      - name: Run Top10 Engine + Step7
        shell: bash
        env:
          TRADE_DATE: ${{ steps.resolve_date.outputs.trade_date }}
        run: |
          set -euo pipefail
          echo "Running engine with TRADE_DATE=$TRADE_DATE"

          if [ -f "run.py" ]; then
            python run.py
          else
            python -m a_top10
          fi

          # ✅ Step7：自学习闭环（会写 outputs/learning 报告 + models/*.joblib）
          python -m a_top10.steps.step7_self_learning

      - name: Step7 artifacts check + stamp (always)
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          td="${{ steps.resolve_date.outputs.trade_date }}"
          rid="${{ github.run_id }}"

          echo "====== LIST outputs (top) ======"
          ls -la outputs || true

          echo "====== LIST outputs/learning ======"
          ls -la outputs/learning || true

          echo "====== LIST models (repo root) ======"
          ls -la models || true

          # ✅ 关键：写一个每次必变的哨兵文件，保证你在仓库“看得见 Step7 跑过”
          mkdir -p outputs/learning
          echo "run_id=${rid} trade_date=${td} utc=$(date -u '+%Y-%m-%dT%H:%M:%SZ')" > "outputs/learning/_last_run_${td}_${rid}.txt"
          echo "STAMP written: outputs/learning/_last_run_${td}_${rid}.txt"

      - name: Commit & push outputs + models (always)
        if: always()
        shell: bash
        run: |
          set -euo pipefail

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # ✅ 只提交 outputs + models（避免把 _warehouse 等目录误提交）
          git add -A outputs models

          echo "====== git status (after add outputs+models) ======"
          git status --porcelain

          if [ -z "$(git status --porcelain)" ]; then
            echo "No changes to commit."
            exit 0
          fi

          td="${{ steps.resolve_date.outputs.trade_date }}"
          rid="${{ github.run_id }}"

          git commit -m "auto: run top10 ${td} (run=${rid})" || true
          git pull --rebase origin main || true
          git push || true
